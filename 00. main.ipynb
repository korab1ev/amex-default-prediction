{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "asttokens               3.0.0\n",
      "colorama                0.4.6\n",
      "comm                    0.2.2\n",
      "debugpy                 1.8.13\n",
      "decorator               5.2.1\n",
      "executing               2.2.0\n",
      "ipykernel               6.29.5\n",
      "ipython                 9.0.2\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.7.2\n",
      "lightgbm                4.6.0\n",
      "matplotlib-inline       0.1.7\n",
      "nest-asyncio            1.6.0\n",
      "numpy                   2.2.4\n",
      "packaging               24.2\n",
      "pandas                  2.2.3\n",
      "parso                   0.8.4\n",
      "pip                     25.0.1\n",
      "platformdirs            4.3.7\n",
      "prompt_toolkit          3.0.50\n",
      "psutil                  7.0.0\n",
      "pure_eval               0.2.3\n",
      "Pygments                2.19.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2025.2\n",
      "pywin32                 310\n",
      "pyzmq                   26.4.0\n",
      "scipy                   1.15.2\n",
      "six                     1.17.0\n",
      "stack-data              0.6.3\n",
      "tornado                 6.4.2\n",
      "traitlets               5.14.3\n",
      "tzdata                  2025.2\n",
      "wcwidth                 0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "asttokens               3.0.0\n",
      "colorama                0.4.6\n",
      "comm                    0.2.2\n",
      "debugpy                 1.8.13\n",
      "decorator               5.2.1\n",
      "executing               2.2.0\n",
      "ipykernel               6.29.5\n",
      "ipython                 9.0.2\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.7.2\n",
      "lightgbm                4.6.0\n",
      "matplotlib-inline       0.1.7\n",
      "nest-asyncio            1.6.0\n",
      "numpy                   2.2.4\n",
      "packaging               24.2\n",
      "parso                   0.8.4\n",
      "pip                     25.0.1\n",
      "platformdirs            4.3.7\n",
      "prompt_toolkit          3.0.50\n",
      "psutil                  7.0.0\n",
      "pure_eval               0.2.3\n",
      "Pygments                2.19.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pywin32                 310\n",
      "pyzmq                   26.4.0\n",
      "scipy                   1.15.2\n",
      "six                     1.17.0\n",
      "stack-data              0.6.3\n",
      "tornado                 6.4.2\n",
      "traitlets               5.14.3\n",
      "wcwidth                 0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from typing import List\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from typing import List\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n",
    "\n",
    "\n",
    "def get_train_data(TRAIN_PATH: str) -> pd.DataFrame:\n",
    "    '''Returns train dataset'''\n",
    "    df_train = pd.read_parquet(TRAIN_PATH)\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def get_test_data(TEST_PATH: str) -> pd.DataFrame:\n",
    "    '''Returns test dataset'''\n",
    "    df_test = pd.read_parquet(TEST_PATH)\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def get_target(TARGET_PATH: str) -> pd.DataFrame:\n",
    "    '''Retruns dataset with train targets'''\n",
    "    df_train_target = pd.read_csv(TARGET_PATH)\n",
    "    return df_train_target\n",
    "\n",
    "\n",
    "def get_train_data_with_target_merged(df_train: pd.DataFrame, df_train_target: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Retruns train dataset with target variable merged'''\n",
    "    df_train_w_target = (\n",
    "        df_train\n",
    "        .merge(df_train_target,\n",
    "            on='customer_ID',\n",
    "            how='left'\n",
    "        )\n",
    "    )\n",
    "    # df_train_w_target.groupby('target', dropna=False).count()['customer_ID']\n",
    "    '''\n",
    "    target\n",
    "    0    4153582\n",
    "    1    1377869\n",
    "    Name: customer_ID, dtype: int64    \n",
    "    '''\n",
    "    return df_train_w_target\n",
    "\n",
    "\n",
    "def get_all_features(df: pd.DataFrame) -> List:\n",
    "    '''Returns list of all features from the dataset'''\n",
    "    return list(df)\n",
    "\n",
    "\n",
    "def get_cat_features() -> List:\n",
    "    '''Returns list of categorical features from the dataset'''\n",
    "    cat_features = ['B_30', 'B_38', 'D_114', \n",
    "                    'D_116', 'D_117', 'D_120', \n",
    "                    'D_126', 'D_63', 'D_64', \n",
    "                    'D_66', 'D_68']\n",
    "    \n",
    "    return cat_features\n",
    "\n",
    "\n",
    "def get_num_features(all_features: List, cat_features: List) -> List:\n",
    "    '''Returns list of all numerical features from the dataset'''\n",
    "    num_feats = [col for col in all_features if col not in cat_features + ['customer_ID', 'S_2', 'target']]\n",
    "\n",
    "    return num_feats\n",
    "\n",
    "\n",
    "def get_df_w_aggrs(df: pd.DataFrame, numerical_features: List) ->  pd.DataFrame:\n",
    "    '''Returns dataframe with generated aggregates based on numerical features'''\n",
    "\n",
    "    cid = pd.Categorical(df.pop('customer_ID'), ordered=True)\n",
    "    last = (cid != np.roll(cid, -1)) # mask for last statement of every customer\n",
    "\n",
    "    df_min = (df\n",
    "        .groupby(cid)\n",
    "        .min()[numerical_features]\n",
    "        .rename(columns={f: f\"{f}_min\" for f in numerical_features})\n",
    "    )\n",
    "    print(df_min.shape)\n",
    "\n",
    "    df_max = (df\n",
    "        .groupby(cid)\n",
    "        .max()[numerical_features]\n",
    "        .rename(columns={f: f\"{f}_max\" for f in numerical_features})\n",
    "    )\n",
    "    print(df_max.shape)\n",
    "\n",
    "    df_avg = (df\n",
    "        .drop('S_2', axis='columns')\n",
    "        .groupby(cid)\n",
    "        .mean()[numerical_features]\n",
    "        .rename(columns={f: f\"{f}_avg\" for f in numerical_features})\n",
    "    )\n",
    "    print(df_avg.shape)\n",
    "\n",
    "    df_last = (df\n",
    "        .loc[last, numerical_features]\n",
    "        .rename(columns={f: f\"{f}_last\" for f in numerical_features})\n",
    "        .set_index(np.asarray(cid[last]))\n",
    "    )\n",
    "    print(df_last.shape)\n",
    "\n",
    "    df_aggrs = (pd.concat([df_min, df_max, df_avg, df_last], axis=1)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'customer_ID'})\n",
    "    )\n",
    "    print(df_aggrs.shape)\n",
    "\n",
    "    '''\n",
    "    del df, df_min, df_max, df_avg, cid, last\n",
    "    gc.collect()\n",
    "    '''\n",
    "    return df_aggrs\n",
    "\n",
    "\n",
    "def check_zapolnenie(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Returns pd.DataFrame with isNotNullShare of each column of given df'''\n",
    "    # Calculate percent of not null share each column \n",
    "    col_pct_notNull = [] \n",
    "    for col in df.columns: \n",
    "        percent_notNull = np.mean(~df[col].isnull())*100 \n",
    "        col_pct_notNull.append([col, percent_notNull]) \n",
    "        \n",
    "    col_pct_notNull_df = pd.DataFrame(col_pct_notNull, columns = ['column_name','isNotNullShare']).sort_values(by = 'isNotNullShare', ascending = False) \n",
    "    #print(col_pct_notNull_df)\n",
    "    return col_pct_notNull_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_train = \u001b[43mget_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data/train.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m all_features = get_all_features(df_train)\n\u001b[32m      4\u001b[39m cat_features = get_cat_features()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mget_train_data\u001b[39m\u001b[34m(TRAIN_PATH)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_train_data\u001b[39m(TRAIN_PATH: \u001b[38;5;28mstr\u001b[39m) -> pd.DataFrame:\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Returns train dataset'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_train\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:651\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    500\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    508\u001b[39m     **kwargs,\n\u001b[32m    509\u001b[39m ) -> DataFrame:\n\u001b[32m    510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    512\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    654\u001b[39m         msg = (\n\u001b[32m    655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:67\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     65\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "df_train = get_train_data(TRAIN_PATH='./data/train.parquet')\n",
    "\n",
    "all_features = get_all_features(df_train)\n",
    "cat_features = get_cat_features()\n",
    "num_features = get_num_features(all_features, cat_features)\n",
    "# len(all_features), len(cat_features), len(num_features) -> (190, 11, 178)\n",
    "\n",
    "df_train_agg = get_df_w_aggrs(df=df_train, numerical_features=num_features)\n",
    "df_train_target = get_target(TARGET_PATH='./data/train_labels.csv')\n",
    "df_train = get_train_data_with_target_merged(df_train=df_train_agg, df_train_target=df_train_target)\n",
    "\n",
    "'''\n",
    "df_train.target.value_counts()\n",
    "target\n",
    "0    340085\n",
    "1    118828\n",
    "Name: count, dtype: int64    \n",
    "'''\n",
    "\n",
    "# df_test = get_test_data(TEST_PATH='./data/test.parquet')\n",
    "# df_test = get_df_w_aggrs(df=df_test, numerical_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 710)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amex_metric_calculated(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000015785\n"
     ]
    }
   ],
   "source": [
    "print(get_amex_metric_calculated(y_true=df_train.target, y_pred=df_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier, log_evaluation\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, log_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "458908    0\n",
       "458909    0\n",
       "458910    0\n",
       "458911    1\n",
       "458912    0\n",
       "Name: target, Length: 458913, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_vkr_environment",
   "language": "python",
   "name": "my_vkr_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
