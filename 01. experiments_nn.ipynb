{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73039bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run import_libs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25242305",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b26bac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 151)\n",
      "(458913, 151)\n",
      "(458913, 151)\n",
      "(458913, 162)\n",
      "(458913, 616)\n"
     ]
    }
   ],
   "source": [
    "df_train = get_train_data(TRAIN_PATH='./data/train.parquet')\n",
    "num_features = pd.read_csv(\"num_feats_after_filtering.csv\")[\"0\"].to_list()\n",
    "\n",
    "df_train_agg = get_df_w_aggrs(df=df_train, feats=num_features)\n",
    "df_train_target = get_target(TARGET_PATH='./data/train_labels.csv')\n",
    "df_train = get_train_data_with_target_merged(df_train=df_train_agg, df_train_target=df_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08b35bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924621, 151)\n",
      "(924621, 151)\n",
      "(924621, 151)\n",
      "(924621, 162)\n",
      "(924621, 616)\n"
     ]
    }
   ],
   "source": [
    "df_test = get_test_data(TEST_PATH='./data/test.parquet')\n",
    "df_test = get_df_w_aggrs(df=df_test, feats=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392b5047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_30_last',\n",
       " 'B_38_last',\n",
       " 'D_114_last',\n",
       " 'D_116_last',\n",
       " 'D_117_last',\n",
       " 'D_120_last',\n",
       " 'D_126_last',\n",
       " 'D_63_last',\n",
       " 'D_64_last',\n",
       " 'D_66_last',\n",
       " 'D_68_last']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features = [f\"{f}_last\" for f in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']]\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff39c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_feats = []\n",
    "delinq_feats = []\n",
    "spend_feats = []\n",
    "balance_feats = []\n",
    "risk_feats = []\n",
    "\n",
    "for feat in list(df_train):\n",
    "    if feat in cat_features:\n",
    "        continue\n",
    "    \n",
    "    if feat[0] == 'P':\n",
    "        #print(feat)\n",
    "        payment_feats.append(feat)\n",
    "    elif feat[0] == 'D':\n",
    "        delinq_feats.append(feat)\n",
    "    elif feat[0] == 'S':\n",
    "        spend_feats.append(feat)\n",
    "    elif feat[0] == 'B':\n",
    "        balance_feats.append(feat)\n",
    "    elif feat[0] == 'R':\n",
    "        risk_feats.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc0986ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(payment_feats) + len(delinq_feats) + len(spend_feats) + len(balance_feats) + len(risk_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "767e6122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = payment_feats + delinq_feats + spend_feats + balance_feats + risk_feats\n",
    "len(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e1ae7",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пока попробуем онли на числовых, без fillna может заработает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2aba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEBUG = False\n",
    "    model = 'tabnet'\n",
    "    N_folds = 5\n",
    "    seed = 42\n",
    "    batch_size = 512\n",
    "    max_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4a6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(seed = CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ea73cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbcdc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using amex metric to evaluate tabnet\n",
    "class Amex_tabnet(Metric):\n",
    "    \n",
    "  def __init__(self):\n",
    "    self._name = 'amex_tabnet'\n",
    "    self._maximize = True\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    amex = get_amex_metric_calculated(y_true, y_pred[:, 1])\n",
    "    return max(amex, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9c04d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09cb5890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  --------------------------------------------------\n",
      "\n",
      "Training:  tabnet\n",
      "\n",
      "  --------------------------------------------------\n",
      "\n",
      "Seed:  42\n",
      "N folds:  5\n",
      "\n",
      "N features:  604\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n ', '-'*50)\n",
    "print('\\nTraining: ', CFG.model)\n",
    "print('\\n ', '-'*50)\n",
    "\n",
    "\n",
    "print('\\nSeed: ', CFG.seed)\n",
    "print('N folds: ', CFG.N_folds)\n",
    "\n",
    "print('\\nN features: ', len(num_features))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daa6fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c10773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66e04729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (458913, 617) (458913,)\n"
     ]
    }
   ],
   "source": [
    "print('Shapes:', train.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24a1110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49479c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_train = \u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m[train_idx]\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "y_train = target.loc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train, y_train = train.loc[train_idx], target.loc[train_idx]\n",
    "    X_valid, y_valid = train.loc[valid_idx], target.loc[valid_idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f015acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "640f8489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75ae51ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86234f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_tr.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c93a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.51859 | val_0_auc: 0.91092 | val_0_accuracy: 0.83555 | val_0_amex_tabnet: 0.5967  |  0:01:04s\n",
      "epoch 1  | loss: 0.35551 | val_0_auc: 0.9277  | val_0_accuracy: 0.86246 | val_0_amex_tabnet: 0.6642  |  0:02:08s\n",
      "epoch 2  | loss: 0.31299 | val_0_auc: 0.93591 | val_0_accuracy: 0.87087 | val_0_amex_tabnet: 0.68442 |  0:03:18s\n",
      "epoch 3  | loss: 0.29255 | val_0_auc: 0.93984 | val_0_accuracy: 0.87697 | val_0_amex_tabnet: 0.7011  |  0:04:25s\n",
      "epoch 4  | loss: 0.28269 | val_0_auc: 0.9412  | val_0_accuracy: 0.87815 | val_0_amex_tabnet: 0.70547 |  0:05:29s\n",
      "epoch 5  | loss: 0.27169 | val_0_auc: 0.94764 | val_0_accuracy: 0.88494 | val_0_amex_tabnet: 0.73266 |  0:06:32s\n",
      "epoch 6  | loss: 0.25838 | val_0_auc: 0.95045 | val_0_accuracy: 0.88843 | val_0_amex_tabnet: 0.7434  |  0:07:36s\n",
      "epoch 7  | loss: 0.25037 | val_0_auc: 0.95238 | val_0_accuracy: 0.89001 | val_0_amex_tabnet: 0.75335 |  0:08:40s\n",
      "epoch 8  | loss: 0.24568 | val_0_auc: 0.95314 | val_0_accuracy: 0.89127 | val_0_amex_tabnet: 0.75683 |  0:09:45s\n",
      "epoch 9  | loss: 0.24329 | val_0_auc: 0.95397 | val_0_accuracy: 0.89262 | val_0_amex_tabnet: 0.76042 |  0:10:50s\n",
      "epoch 10 | loss: 0.24319 | val_0_auc: 0.95555 | val_0_accuracy: 0.89583 | val_0_amex_tabnet: 0.7683  |  0:11:59s\n",
      "epoch 11 | loss: 0.23688 | val_0_auc: 0.95738 | val_0_accuracy: 0.89727 | val_0_amex_tabnet: 0.77417 |  0:13:06s\n",
      "epoch 12 | loss: 0.23358 | val_0_auc: 0.95809 | val_0_accuracy: 0.89871 | val_0_amex_tabnet: 0.77662 |  0:14:14s\n",
      "epoch 13 | loss: 0.23115 | val_0_auc: 0.9586  | val_0_accuracy: 0.90053 | val_0_amex_tabnet: 0.77859 |  0:15:23s\n",
      "epoch 14 | loss: 0.22755 | val_0_auc: 0.95936 | val_0_accuracy: 0.9008  | val_0_amex_tabnet: 0.77996 |  0:16:32s\n",
      "epoch 15 | loss: 0.23307 | val_0_auc: 0.9584  | val_0_accuracy: 0.89956 | val_0_amex_tabnet: 0.77751 |  0:17:43s\n",
      "epoch 16 | loss: 0.2313  | val_0_auc: 0.95858 | val_0_accuracy: 0.89799 | val_0_amex_tabnet: 0.77758 |  0:19:05s\n",
      "epoch 17 | loss: 0.22933 | val_0_auc: 0.95919 | val_0_accuracy: 0.90093 | val_0_amex_tabnet: 0.78201 |  0:20:34s\n",
      "epoch 18 | loss: 0.22727 | val_0_auc: 0.95953 | val_0_accuracy: 0.90119 | val_0_amex_tabnet: 0.78224 |  0:22:06s\n",
      "epoch 19 | loss: 0.22533 | val_0_auc: 0.95984 | val_0_accuracy: 0.90152 | val_0_amex_tabnet: 0.78259 |  0:23:44s\n",
      "epoch 20 | loss: 0.22932 | val_0_auc: 0.95915 | val_0_accuracy: 0.90043 | val_0_amex_tabnet: 0.78161 |  0:25:29s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     38\u001b[39m model = TabNetClassifier(n_d = \u001b[32m32\u001b[39m,\n\u001b[32m     39\u001b[39m                          n_a = \u001b[32m32\u001b[39m,\n\u001b[32m     40\u001b[39m                          n_steps = \u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m                          mask_type = \u001b[33m'\u001b[39m\u001b[33mentmax\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     55\u001b[39m                          seed = CFG.seed)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m## train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#np.array(y_tr.values.ravel()),\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m          \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_va\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_va\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m          \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m          \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAmex_tabnet\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Last metric is used for early stopping\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Saving best model\u001b[39;00m\n\u001b[32m     68\u001b[39m saving_path_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[39m, in \u001b[36mTabModel.fit\u001b[39m\u001b[34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_epochs):\n\u001b[32m    254\u001b[39m \n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_epoch_begin(epoch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[39m, in \u001b[36mTabModel._train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_begin(batch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     batch_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_end(batch_idx, batch_logs)\n\u001b[32m    493\u001b[39m epoch_logs = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._optimizer.param_groups[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:527\u001b[39m, in \u001b[36mTabModel._train_batch\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.network.parameters():\n\u001b[32m    525\u001b[39m     param.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_loss(output, y)\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:616\u001b[39m, in \u001b[36mTabNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    615\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedder(x)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:492\u001b[39m, in \u001b[36mTabNetNoEmbeddings.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    491\u001b[39m     res = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     steps_output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     res = torch.sum(torch.stack(steps_output, dim=\u001b[32m0\u001b[39m), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_multi_task:\n\u001b[32m    496\u001b[39m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:181\u001b[39m, in \u001b[36mTabNetEncoder.forward\u001b[39m\u001b[34m(self, x, prior)\u001b[39m\n\u001b[32m    179\u001b[39m M_feature_level = torch.matmul(M, \u001b[38;5;28mself\u001b[39m.group_attention_matrix)\n\u001b[32m    180\u001b[39m masked_x = torch.mul(M_feature_level, x)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeat_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m d = ReLU()(out[:, : \u001b[38;5;28mself\u001b[39m.n_d])\n\u001b[32m    183\u001b[39m steps_output.append(d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:737\u001b[39m, in \u001b[36mFeatTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.specifics(x)\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:774\u001b[39m, in \u001b[36mGLU_Block.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    772\u001b[39m scale = torch.sqrt(torch.FloatTensor([\u001b[32m0.5\u001b[39m]).to(x.device))\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first:  \u001b[38;5;66;03m# the first layer of the block has no scale multiplication\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglu_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m     layers_left = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_glu)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:804\u001b[39m, in \u001b[36mGLU_Layer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    803\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc(x)\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m     out = torch.mul(x[:, : \u001b[38;5;28mself\u001b[39m.output_dim], torch.sigmoid(x[:, \u001b[38;5;28mself\u001b[39m.output_dim :]))\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:35\u001b[39m, in \u001b[36mGBN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     chunks = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     res = [\u001b[38;5;28mself\u001b[39m.bn(x_) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(res, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create out of folds array\n",
    "oof_predictions = np.zeros((train.shape[0]))\n",
    "test_predictions = np.zeros(test.shape[0])\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances[\"feature\"] = train.columns.tolist()\n",
    "stats = pd.DataFrame()\n",
    "explain_matrices = []\n",
    "masks_ =[]\n",
    "\n",
    "target_col = 'target'\n",
    "group_col = 'customer_ID'\n",
    "\n",
    "target, groups = df_train[target_col].values, df_train[group_col].values\n",
    "    \n",
    "# kfold = StratifiedKFold(n_splits = CFG.N_folds, shuffle=True, random_state = CFG.seed)\n",
    "sgkf = StratifiedGroupKFold(CFG.N_folds, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "# for tr_idx, va_idx in sgkf.split(df_train[[group_col, target_col]], y, groups):\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(\n",
    "                sgkf.split(df_train[[group_col, target_col]], target, groups)):\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    ## DEBUG MODE\n",
    "    if CFG.DEBUG == True:\n",
    "        if fold > 0:\n",
    "            print('\\nDEBUG mode activated: Will train only one fold...\\n')\n",
    "            break      \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    X_tr, X_va = df_train.iloc[tr_idx][num_features], df_train.iloc[va_idx][num_features]\n",
    "    y_tr, y_va = target[tr_idx], target[va_idx]    \n",
    "\n",
    "    # X_train, y_train = train.loc[train_idx], target.loc[train_idx]\n",
    "    # X_valid, y_valid = train.loc[valid_idx], target.loc[valid_idx]        \n",
    "        \n",
    "    model = TabNetClassifier(n_d = 32,\n",
    "                             n_a = 32,\n",
    "                             n_steps = 3,\n",
    "                             gamma = 1.3,\n",
    "                             n_independent = 2,\n",
    "                             n_shared = 2,\n",
    "                             momentum = 0.02,\n",
    "                             clip_value = None,\n",
    "                             lambda_sparse = 1e-3,\n",
    "                             optimizer_fn = torch.optim.Adam,\n",
    "                             optimizer_params = dict(lr = 1e-3, weight_decay=1e-3),\n",
    "                             scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                             scheduler_params = {'T_0':5,\n",
    "                                                 'eta_min':1e-4,\n",
    "                                                 'T_mult':1,\n",
    "                                                 'last_epoch':-1},\n",
    "                             mask_type = 'entmax',\n",
    "                             seed = CFG.seed)\n",
    "    \n",
    "    ## train\n",
    "    model.fit(np.array(X_tr),\n",
    "              #np.array(y_tr.values.ravel()),\n",
    "              y_tr,\n",
    "              eval_set = [(np.array(X_va), y_va)],\n",
    "              max_epochs = CFG.max_epochs,\n",
    "              patience = 50,\n",
    "              batch_size = CFG.batch_size,\n",
    "              eval_metric = ['auc', 'accuracy', Amex_tabnet]) # Last metric is used for early stopping\n",
    "    \n",
    "    # Saving best model\n",
    "    saving_path_name = f\"./fold{fold}\"\n",
    "    saved_filepath = model.save_model(saving_path_name)\n",
    "    \n",
    "    # model explanability\n",
    "    explain_matrix, masks = model.explain(X_va.values)\n",
    "    explain_matrices.append(explain_matrix)\n",
    "    masks_.append(masks[0])\n",
    "    masks_.append(masks[1])\n",
    "    \n",
    "    # Inference\n",
    "    oof_predictions[va_idx] = model.predict_proba(X_va.values)[:, 1]\n",
    "    \n",
    "    #if CFG\n",
    "    # logodds function\n",
    "    \n",
    "    test_predictions += model.predict_proba(test.values)[:, 1]/5\n",
    "    feature_importances[f\"importance_fold{fold}+1\"] = model.feature_importances_\n",
    "    \n",
    "    # Loss , metric tracking\n",
    "    stats[f'fold{fold+1}_train_loss'] = model.history['loss']\n",
    "    stats[f'fold{fold+1}_val_metric'] = model.history['val_0_amex_tabnet']\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    time_delta = np.round((end - start)/60, 2)\n",
    "     \n",
    "    print(f'\\nFold {fold+1}/{CFG.N_folds} | {time_delta:.2f} min')\n",
    "\n",
    "    ### free memory\n",
    "    del X_train, y_train\n",
    "    del X_valid, y_valid\n",
    "    gc.collect()\n",
    "\n",
    "print(f'OOF score across folds: {get_amex_metric_calculated(target, oof_predictions.flatten())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c8fec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b25ef3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2485213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b3528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9898f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d63e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
