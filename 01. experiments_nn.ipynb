{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73039bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run import_libs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25242305",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26bac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 151)\n",
      "(458913, 151)\n",
      "(458913, 151)\n",
      "(458913, 162)\n",
      "(458913, 616)\n"
     ]
    }
   ],
   "source": [
    "df_train = get_train_data(TRAIN_PATH='./data/train.parquet')\n",
    "num_features = pd.read_csv(\"num_feats_after_filtering.csv\")[\"0\"].to_list()\n",
    "\n",
    "df_train_agg = get_df_w_aggrs(df=df_train, feats=num_features)\n",
    "df_train_target = get_target(TARGET_PATH='./data/train_labels.csv')\n",
    "df_train = get_train_data_with_target_merged(df_train=df_train_agg, df_train_target=df_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b35bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924621, 151)\n",
      "(924621, 151)\n",
      "(924621, 151)\n",
      "(924621, 162)\n",
      "(924621, 616)\n"
     ]
    }
   ],
   "source": [
    "df_test = get_test_data(TEST_PATH='./data/test.parquet')\n",
    "df_test = get_df_w_aggrs(df=df_test, feats=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392b5047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_30_last',\n",
       " 'B_38_last',\n",
       " 'D_114_last',\n",
       " 'D_116_last',\n",
       " 'D_117_last',\n",
       " 'D_120_last',\n",
       " 'D_126_last',\n",
       " 'D_63_last',\n",
       " 'D_64_last',\n",
       " 'D_66_last',\n",
       " 'D_68_last']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features = [f\"{f}_last\" for f in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']]\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff39c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_feats = []\n",
    "delinq_feats = []\n",
    "spend_feats = []\n",
    "balance_feats = []\n",
    "risk_feats = []\n",
    "\n",
    "for feat in list(df_train):\n",
    "    if feat in cat_features:\n",
    "        continue\n",
    "    \n",
    "    if feat[0] == 'P':\n",
    "        #print(feat)\n",
    "        payment_feats.append(feat)\n",
    "    elif feat[0] == 'D':\n",
    "        delinq_feats.append(feat)\n",
    "    elif feat[0] == 'S':\n",
    "        spend_feats.append(feat)\n",
    "    elif feat[0] == 'B':\n",
    "        balance_feats.append(feat)\n",
    "    elif feat[0] == 'R':\n",
    "        risk_feats.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0986ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(payment_feats) + len(delinq_feats) + len(spend_feats) + len(balance_feats) + len(risk_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767e6122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = payment_feats + delinq_feats + spend_feats + balance_feats + risk_feats\n",
    "len(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e1ae7",
   "metadata": {},
   "source": [
    "### load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2aba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEBUG = False\n",
    "    model = 'tabnet'\n",
    "    N_folds = 5\n",
    "    seed = 42\n",
    "    batch_size = 512\n",
    "    max_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa4a6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(seed = CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ea73cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory().percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcdc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using amex metric to evaluate tabnet\n",
    "class Amex_tabnet(Metric):\n",
    "    \n",
    "  def __init__(self):\n",
    "    self._name = 'amex_tabnet'\n",
    "    self._maximize = True\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    amex = get_amex_metric_calculated(y_true, y_pred[:, 1])\n",
    "    return max(amex, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9c04d",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09cb5890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  --------------------------------------------------\n",
      "\n",
      "Training:  tabnet\n",
      "\n",
      "  --------------------------------------------------\n",
      "\n",
      "Seed:  42\n",
      "N folds:  5\n",
      "\n",
      "N features:  604\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n ', '-'*50)\n",
    "print('\\nTraining: ', CFG.model)\n",
    "print('\\n ', '-'*50)\n",
    "\n",
    "print('\\nSeed: ', CFG.seed)\n",
    "print('N folds: ', CFG.N_folds)\n",
    "\n",
    "print('\\nN features: ', len(num_features))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa6fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (458913, 617) (924621, 616)\n"
     ]
    }
   ],
   "source": [
    "train = df_train.fillna(0)\n",
    "test = df_test.fillna(0)\n",
    "\n",
    "print('Shapes:', train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c93a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.51859 | val_0_auc: 0.91092 | val_0_accuracy: 0.83555 | val_0_amex_tabnet: 0.5967  |  0:00:56s\n",
      "epoch 1  | loss: 0.35551 | val_0_auc: 0.9277  | val_0_accuracy: 0.86246 | val_0_amex_tabnet: 0.6642  |  0:01:52s\n",
      "epoch 2  | loss: 0.31299 | val_0_auc: 0.93591 | val_0_accuracy: 0.87087 | val_0_amex_tabnet: 0.68442 |  0:02:49s\n",
      "Stop training because you reached max_epochs = 3 with best_epoch = 2 and best_val_0_amex_tabnet = 0.68442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./fold0.zip\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     81\u001b[39m oof_predictions[va_idx] = model.predict_proba(X_va.values)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m#if CFG\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# logodds function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m test_predictions += \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]/\u001b[32m5\u001b[39m\n\u001b[32m     87\u001b[39m feature_importances[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimportance_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m+1\u001b[39m\u001b[33m\"\u001b[39m] = model.feature_importances_\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Loss , metric tracking\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_model.py:107\u001b[39m, in \u001b[36mTabNetClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    100\u001b[39m     dataloader = DataLoader(\n\u001b[32m    101\u001b[39m         PredictDataset(X),\n\u001b[32m    102\u001b[39m         batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m    103\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    106\u001b[39m results = []\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_nb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_loss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oobur\\Projects\\spbu_master\\sem4\\vkr\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:283\u001b[39m, in \u001b[36mcollate_numpy_array_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern.search(elem.dtype.str) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem.dtype))\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch.as_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map=collate_fn_map)\n",
      "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "# X_train = train.loc[train_idx]\n",
    "# y_train = target.loc[train_idx]\n",
    "\n",
    "# Create out of folds array\n",
    "oof_predictions = np.zeros((train.shape[0]))\n",
    "test_predictions = np.zeros(test.shape[0])\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances[\"feature\"] = train.columns.tolist()\n",
    "stats = pd.DataFrame()\n",
    "explain_matrices = []\n",
    "masks_ =[]\n",
    "\n",
    "target_col = 'target'\n",
    "group_col = 'customer_ID'\n",
    "\n",
    "target, groups = df_train[target_col].values, df_train[group_col].values\n",
    "    \n",
    "# kfold = StratifiedKFold(n_splits = CFG.N_folds, shuffle=True, random_state = CFG.seed)\n",
    "sgkf = StratifiedGroupKFold(CFG.N_folds, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "# for tr_idx, va_idx in sgkf.split(df_train[[group_col, target_col]], y, groups):\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(\n",
    "                sgkf.split(df_train[[group_col, target_col]], target, groups)):\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    ## DEBUG MODE\n",
    "    if CFG.DEBUG == True:\n",
    "        if fold > 0:\n",
    "            print('\\nDEBUG mode activated: Will train only one fold...\\n')\n",
    "            break      \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    X_tr, X_va = df_train.iloc[tr_idx][num_features], df_train.iloc[va_idx][num_features]\n",
    "    y_tr, y_va = target[tr_idx], target[va_idx]    \n",
    "\n",
    "    # X_train, y_train = train.loc[train_idx], target.loc[train_idx]\n",
    "    # X_valid, y_valid = train.loc[valid_idx], target.loc[valid_idx]        \n",
    "        \n",
    "    model = TabNetClassifier(n_d = 32,\n",
    "                             n_a = 32,\n",
    "                             n_steps = 3,\n",
    "                             gamma = 1.3,\n",
    "                             n_independent = 2,\n",
    "                             n_shared = 2,\n",
    "                             momentum = 0.02,\n",
    "                             clip_value = None,\n",
    "                             lambda_sparse = 1e-3,\n",
    "                             optimizer_fn = torch.optim.Adam,\n",
    "                             optimizer_params = dict(lr = 1e-3, weight_decay=1e-3),\n",
    "                             scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                             scheduler_params = {'T_0':5,\n",
    "                                                 'eta_min':1e-4,\n",
    "                                                 'T_mult':1,\n",
    "                                                 'last_epoch':-1},\n",
    "                             mask_type = 'entmax',\n",
    "                             seed = CFG.seed)\n",
    "    \n",
    "    ## train\n",
    "    model.fit(np.array(X_tr),\n",
    "              #np.array(y_tr.values.ravel()),\n",
    "              y_tr,\n",
    "              eval_set = [(np.array(X_va), y_va)],\n",
    "              max_epochs = 3, # CFG.max_epochs\n",
    "              patience = 50,\n",
    "              batch_size = CFG.batch_size,\n",
    "              eval_metric = ['auc', 'accuracy', Amex_tabnet]) # Last metric is used for early stopping\n",
    "    \n",
    "    # Saving best model\n",
    "    saving_path_name = f\"./fold{fold}\"\n",
    "    saved_filepath = model.save_model(saving_path_name)\n",
    "    \n",
    "    # model explanability\n",
    "    explain_matrix, masks = model.explain(X_va.values)\n",
    "    explain_matrices.append(explain_matrix)\n",
    "    masks_.append(masks[0])\n",
    "    masks_.append(masks[1])\n",
    "    \n",
    "    # Inference\n",
    "    oof_predictions[va_idx] = model.predict_proba(X_va.values)[:, 1]\n",
    "    \n",
    "    #if CFG\n",
    "    # logodds function\n",
    "    \n",
    "    test_predictions += model.predict_proba(test.values)[:, 1]/5\n",
    "    feature_importances[f\"importance_fold{fold}+1\"] = model.feature_importances_\n",
    "    \n",
    "    # Loss , metric tracking\n",
    "    stats[f'fold{fold+1}_train_loss'] = model.history['loss']\n",
    "    stats[f'fold{fold+1}_val_metric'] = model.history['val_0_amex_tabnet']\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    time_delta = np.round((end - start)/60, 2)\n",
    "     \n",
    "    print(f'\\nFold {fold+1}/{CFG.N_folds} | {time_delta:.2f} min')\n",
    "\n",
    "    ### free memory\n",
    "    del X_train, y_train\n",
    "    del X_valid, y_valid\n",
    "    gc.collect()\n",
    "\n",
    "print(f'OOF score across folds: {get_amex_metric_calculated(target, oof_predictions.flatten())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2485213",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE = True\n",
    "\n",
    "if INFERENCE:\n",
    "    sub = pd.DataFrame({'customer_ID': df_test.customer_ID,\n",
    "                        'prediction': np.mean(test_predictions, axis=0)})\n",
    "    sub.to_csv('submission_lr_woe_features.csv', index=False)\n",
    "    display(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b3528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9898f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d63e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
